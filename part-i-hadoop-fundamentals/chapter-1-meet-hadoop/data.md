# Data!

We live in the data age. It’s not easy to [measure ](/测量)the <a href="#" title="abc">total</a> volume of data stored electronically, but an [IDC](/国际文献资料中心) estimate put the size of the “digital universe” at 4.4 [zettabytes ](/泽字节%28ZB%29)in 2013 and is forecasting a tenfold growth by 2020 to 44 zettabytes.[^1] A zettabyte is 1021 bytes, or [equivalently ](/相等地)one thousand [exabytes](/艾字节%28EB%29), one million [petabytes](/拍字节%28PB%29), or one billion [terabytes](/太字节%28TB%29). That’s more than one disk drive for every person in the world.

This flood of data is coming from many sources. Consider the following:[^2]

* The New York Stock Exchange generates about 4−5 terabytes of data per day.
* Facebook hosts more than 240 billion photos, growing at 7 petabytes per month.
* Ancestry.com, the genealogy site, stores around 10 petabytes of data.
* The Internet Archive stores around 18.5 petabytes of data.
* The Large Hadron Collider near Geneva, Switzerland, produces about 30 petabytes of data per year.

So there’s a lot of data out there. But you are probably wondering how it affects you. Most of the data is locked up in the largest web properties \(like search engines\) or in scientific or financial institutions, isn’t it? Does the advent of big data affect smaller organizations or individuals?

I argue that it does. Take photos, for example. My wife’s grandfather was an avid pho‐ tographer and took photographs throughout his adult life. His entire corpus of mediumformat, slide, and 35mm film, when scanned in at high resolution, occupies around 10 gigabytes. Compare this to the digital photos my family took in 2008, which take up about 5 gigabytes of space. My family is producing photographic data at 35 times the rate my wife’s grandfather’s did, and the rate is increasing every year as it becomes easier to take more and more photos.

More generally, the digital streams that individuals are producing are growing apace. **Microsoft Research’s MyLifeBits project** gives a glimpse of the archiving of personal information that may become commonplace in the near future. MyLifeBits was an ex‐ periment where an individual’s interactions—phone calls, emails, documents—were captured electronically and stored for later access. The data gathered included a photo taken every minute, which resulted in an overall data volume of 1 gigabyte per month. When storage costs come down enough to make it feasible to store continuous audio and video, the data volume for a future MyLifeBits service will be many times that.

The trend is for every individual’s data footprint to grow, but perhaps more significantly, the amount of data generated by machines as a part of the Internet of Things will be even greater than that generated by people. Machine logs, RFID readers, sensor net‐ works, vehicle GPS traces, retail transactions—all of these contribute to the growing mountain of data.

The volume of data being made publicly available increases every year, too. Organiza‐ tions no longer have to merely manage their own data; success in the future will be dictated to a large extent by their ability to extract value from other organizations’ data.

Initiatives such as **Public Data Sets on Amazon Web Services** and **Infochimps.org** exist to foster the “information commons,” where data can be freely \(or for a modest price\) shared for anyone to download and analyze. Mashups between different information sources make for unexpected and hitherto unimaginable applications.

Take, for example, the **Astrometry.net project**, which watches the Astrometry group on Flickr for new photos of the night sky. It analyzes each image and identifies which part of the sky it is from, as well as any interesting celestial bodies, such as stars or galaxies. This project shows the kinds of things that are possible when data \(in this case, tagged photographic images\) is made available and used for something \(image analysis\) that was not anticipated by the creator.

It has been said that “more data usually beats better algorithms,” which is to say that for some problems \(such as recommending movies or music based on past preferences\), however fiendish your algorithms, often they can be beaten simply by having more data \(and a less sophisticated algorithm\).[^3]

The good news is that big data is here. The bad news is that we are struggling to store and analyze it.

[^1]: These statistics were reported in a study entitled **"The Digital Universe of Opportunities: Rich Data and the Increasing Value of the Internet of Things."**

[^2]: All figures are from 2013 or 2014. For more information, see Tom Groenfeldt, **“At NYSE, The Data Deluge Overwhelms Traditional Databases”**; Rich Miller, **“Facebook Builds Exabyte Data Centers for Cold Stor‐ age”**; Ancestry.com’s _“Company Facts”**; Archive.org’s **“Petabox”**; and the **Worldwide LHC Computing Grid project’s welcome page\*_.

[^3]: The quote is from Anand Rajaraman’s blog post **“More data usually beats better algorithms,”** in which he writes about the Netflix Challenge. Alon Halevy, Peter Norvig, and Fernando Pereira make the same point in **“The Unreasonable Effectiveness of Data,”** IEEE Intelligent Systems, March/April 2009.

